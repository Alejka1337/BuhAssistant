"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑ liga.net
"""
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
import asyncio


class LigaNetArticle:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞—Ç—å–∏ liga.net"""
    def __init__(self, title: str, url: str, category: str = None, source: str = "liga.net"):
        self.title = title
        self.url = url  # liga.net –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–æ–≤–Ω—ñ URL
        self.category = category
        self.source = source
        self.parsed_at = datetime.utcnow()
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'url': self.url,
            'source': self.source,
            'category': self.category,
            'parsed_at': self.parsed_at.isoformat()
        }
    
    def __repr__(self):
        return f"<LigaNetArticle(title='{self.title[:50]}...', url='{self.url}', category='{self.category}')>"


async def parse_liga_net_page(url: str = 'https://news.liga.net/ua') -> List[LigaNetArticle]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π liga.net
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ LigaNetArticle
    """
    articles = []
    
    try:
        print(f"üì∞ Fetching news from {url}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status != 200:
                    print(f"‚ùå Error: HTTP {response.status}")
                    return articles
                
                html = await response.text()
        
        # –ü–∞—Ä—Å–∏–Ω–≥ HTML
        soup = BeautifulSoup(html, 'lxml')
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        news_container = soup.find('div', class_='news-list-page')
        
        if not news_container:
            print(f"‚ö†Ô∏è News container not found on {url}")
            return articles
        
        # –ò—â–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
        news_cards = news_container.find_all('article', class_='news-card')
        
        print(f"üì∞ Found {len(news_cards)} news cards on {url}")
        
        for card in news_cards:
            try:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                title_link = card.find('a', class_='news-card__title')
                
                if not title_link:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º URL
                article_url = title_link.get('href', '')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ h4
                h4_tag = title_link.find('h4')
                if not h4_tag:
                    continue
                
                title = h4_tag.get_text(strip=True)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                category = None
                badge = card.find('a', class_='news-card__badge')
                if badge:
                    category = badge.get_text(strip=True)
                
                if title and article_url:
                    article = LigaNetArticle(
                        title=title,
                        url=article_url,
                        category=category
                    )
                    articles.append(article)
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
                    cat_info = f" [{category}]" if category else ""
                    print(f"  ‚úÖ Parsed: {title[:60]}...{cat_info}")
                
            except Exception as e:
                print(f"  ‚ùå Error parsing card: {e}")
                continue
        
        print(f"‚úÖ Successfully parsed {len(articles)} articles from {url}")
        
    except asyncio.TimeoutError:
        print(f"‚è∞ Timeout while fetching {url}")
    except Exception as e:
        print(f"‚ùå Error parsing {url}: {e}")
        import traceback
        traceback.print_exc()
    
    return articles


async def crawl_liga_net() -> List[LigaNetArticle]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π liga.net
    
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    """
    url = 'https://news.liga.net/ua'
    
    print(f"üï∑Ô∏è Starting Liga.net crawler...")
    
    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    articles = await parse_liga_net_page(url)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
    unique_articles = {}
    for article in articles:
        if article.url not in unique_articles:
            unique_articles[article.url] = article
    
    print(f"üéâ Liga.net crawler finished: {len(unique_articles)} unique articles")
    return list(unique_articles.values())


class LigaNetCrawler:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π —Å liga.net
    """
    
    def __init__(self):
        pass
    
    def crawl_all(self) -> List[Dict]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è async crawl_liga_net()
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤–º–µ—Å—Ç–æ LigaNetArticle –æ–±—ä–µ–∫—Ç–æ–≤
        """
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            articles = loop.run_until_complete(crawl_liga_net())
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å API
            return [article.to_dict() for article in articles]
        finally:
            loop.close()


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    async def test():
        articles = await crawl_liga_net()
        print(f"\nüìä Total articles: {len(articles)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        by_category = {}
        for article in articles:
            cat = article.category or "–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(article)
        
        print(f"\nüìÇ By category:")
        for cat, arts in by_category.items():
            print(f"  {cat}: {len(arts)} articles")
        
        print(f"\nüì∞ First 5 articles:")
        for i, article in enumerate(articles[:5], 1):
            print(f"{i}. {article.title}")
            print(f"   Category: {article.category or 'N/A'}")
            print(f"   URL: {article.url}")
    
    asyncio.run(test())

