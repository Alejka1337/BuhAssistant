"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å tax.gov.ua
"""
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
import asyncio


class TaxGovUaArticle:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞—Ç—å–∏ tax.gov.ua"""
    def __init__(self, title: str, url: str, source: str = "tax.gov.ua"):
        self.title = title
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–π URL –µ—Å–ª–∏ —ç—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞
        self.url = url if url.startswith('http') else f"https://tax.gov.ua{url}"
        self.source = source
        self.parsed_at = datetime.utcnow()
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'url': self.url,
            'source': self.source,
            'parsed_at': self.parsed_at.isoformat()
        }
    
    def __repr__(self):
        return f"<TaxGovUaArticle(title='{self.title[:50]}...', url='{self.url}')>"


async def parse_tax_gov_ua_page(url: str = 'https://tax.gov.ua/media-tsentr/novini/') -> List[TaxGovUaArticle]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π tax.gov.ua
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ TaxGovUaArticle
    """
    articles = []
    
    try:
        print(f"üì∞ Fetching news from {url}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
        await asyncio.sleep(1)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        import ssl
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status != 200:
                    print(f"‚ùå Error: HTTP {response.status}")
                    print(f"‚ö†Ô∏è Site tax.gov.ua may be protected by CDN (Cloudflare/Akamai)")
                    print(f"üí° Possible solutions:")
                    print(f"   1. Use Selenium/Playwright for browser automation")
                    print(f"   2. Use proxy service")
                    print(f"   3. Contact site administrators for API access")
                    return articles
                
                html = await response.text()
        
        # –ü–∞—Ä—Å–∏–Ω–≥ HTML
        soup = BeautifulSoup(html, 'lxml')
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        news_list = soup.find('div', class_='news__list')
        
        if not news_list:
            print(f"‚ö†Ô∏è News list container not found on {url}")
            return articles
        
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π
        news_items = news_list.find_all('div', class_='news__item')
        
        print(f"üì∞ Found {len(news_items)} items on {url}")
        
        for item in news_items:
            try:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                title_link = item.find('a', class_='news__title')
                
                if not title_link:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ URL
                title = title_link.get_text(strip=True)
                href = title_link.get('href', '')
                
                if title and href:
                    article = TaxGovUaArticle(
                        title=title,
                        url=href
                    )
                    articles.append(article)
                    print(f"  ‚úÖ Parsed: {title[:70]}...")
                
            except Exception as e:
                print(f"  ‚ùå Error parsing item: {e}")
                continue
        
        print(f"‚úÖ Successfully parsed {len(articles)} articles from {url}")
        
    except asyncio.TimeoutError:
        print(f"‚è∞ Timeout while fetching {url}")
    except Exception as e:
        print(f"‚ùå Error parsing {url}: {e}")
        import traceback
        traceback.print_exc()
    
    return articles


async def crawl_tax_gov_ua() -> List[TaxGovUaArticle]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π tax.gov.ua
    
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    """
    url = 'https://tax.gov.ua/media-tsentr/novini/'
    
    print(f"üï∑Ô∏è Starting Tax.gov.ua crawler...")
    
    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    articles = await parse_tax_gov_ua_page(url)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
    unique_articles = {}
    for article in articles:
        if article.url not in unique_articles:
            unique_articles[article.url] = article
    
    print(f"üéâ Tax.gov.ua crawler finished: {len(unique_articles)} unique articles")
    return list(unique_articles.values())


class TaxGovUaCrawler:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π —Å tax.gov.ua
    """
    
    def __init__(self):
        pass
    
    def crawl_all(self) -> List[Dict]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è async crawl_tax_gov_ua()
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤–º–µ—Å—Ç–æ TaxGovUaArticle –æ–±—ä–µ–∫—Ç–æ–≤
        """
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            articles = loop.run_until_complete(crawl_tax_gov_ua())
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å API
            return [article.to_dict() for article in articles]
        finally:
            loop.close()


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    async def test():
        articles = await crawl_tax_gov_ua()
        print(f"\nüìä Total articles: {len(articles)}")
        for i, article in enumerate(articles[:5], 1):
            print(f"{i}. {article.title}")
            print(f"   {article.url}")
    
    asyncio.run(test())

