"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑ buhgalter911.com
"""
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
import asyncio


class Buhgalter911Article:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞—Ç—å–∏ buhgalter911.com"""
    def __init__(self, title: str, url: str, source: str = "buhgalter911.com"):
        self.title = title
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–π URL –µ—Å–ª–∏ —ç—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞
        self.url = url if url.startswith('http') else f"https://buhgalter911.com{url}"
        self.source = source
        self.parsed_at = datetime.utcnow()
    
    def to_dict(self) -> Dict:
        return {
            'title': self.title,
            'url': self.url,
            'source': self.source,
            'parsed_at': self.parsed_at.isoformat()
        }
    
    def __repr__(self):
        return f"<Buhgalter911Article(title='{self.title[:50]}...', url='{self.url}')>"


async def parse_buhgalter911_page(url: str = 'https://buhgalter911.com/uk/news/') -> List[Buhgalter911Article]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π buhgalter911.com
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Buhgalter911Article
    """
    articles = []
    
    try:
        print(f"üì∞ Fetching news from {url}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=30) as response:
                if response.status != 200:
                    print(f"‚ùå Error: HTTP {response.status}")
                    return articles
                
                html = await response.text()
        
        # –ü–∞—Ä—Å–∏–Ω–≥ HTML
        soup = BeautifulSoup(html, 'lxml')
        
        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞–ø—Ä—è–º—É—é
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º lambda –¥–ª—è –ø–æ–∏—Å–∫–∞ div —Å –∫–ª–∞—Å—Å–æ–º, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç 'news__description'
        news_descriptions = soup.find_all('div', class_=lambda x: x and 'news__description' in x)
        
        print(f"üì∞ Found {len(news_descriptions)} news descriptions on {url}")
        
        for description in news_descriptions:
            try:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
                news_link = description.find('a', class_='news__link')
                
                if not news_link:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                href = news_link.get('href', '')
                title = news_link.get_text(strip=True)
                
                if title and href:
                    article = Buhgalter911Article(
                        title=title,
                        url=href
                    )
                    articles.append(article)
                    print(f"  ‚úÖ Parsed: {title[:70]}...")
                
            except Exception as e:
                print(f"  ‚ùå Error parsing description: {e}")
                continue
        
        print(f"‚úÖ Successfully parsed {len(articles)} articles from {url}")
        
    except asyncio.TimeoutError:
        print(f"‚è∞ Timeout while fetching {url}")
    except Exception as e:
        print(f"‚ùå Error parsing {url}: {e}")
        import traceback
        traceback.print_exc()
    
    return articles


async def crawl_buhgalter911() -> List[Buhgalter911Article]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π buhgalter911.com
    
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    """
    url = 'https://buhgalter911.com/uk/news/'
    
    print(f"üï∑Ô∏è Starting Buhgalter911.com crawler...")
    
    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    articles = await parse_buhgalter911_page(url)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
    unique_articles = {}
    for article in articles:
        if article.url not in unique_articles:
            unique_articles[article.url] = article
    
    print(f"üéâ Buhgalter911.com crawler finished: {len(unique_articles)} unique articles")
    return list(unique_articles.values())


class Buhgalter911Crawler:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π —Å buhgalter911.com
    """
    
    def __init__(self):
        pass
    
    def crawl_all(self) -> List[Dict]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è async crawl_buhgalter911()
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤–º–µ—Å—Ç–æ Buhgalter911Article –æ–±—ä–µ–∫—Ç–æ–≤
        """
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            articles = loop.run_until_complete(crawl_buhgalter911())
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å API
            return [article.to_dict() for article in articles]
        finally:
            loop.close()


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    async def test():
        articles = await crawl_buhgalter911()
        print(f"\nüìä Total articles: {len(articles)}")
        
        print(f"\nüì∞ First 10 articles:")
        for i, article in enumerate(articles[:10], 1):
            print(f"{i}. {article.title}")
            print(f"   URL: {article.url}")
    
    asyncio.run(test())

