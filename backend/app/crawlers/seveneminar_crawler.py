"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ 7eminar.ua
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç BeautifulSoup4 + aiohttp –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
"""

import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict
from datetime import datetime, timedelta
import logging
import re

logger = logging.getLogger(__name__)

# URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞
EMINAR_NEWS_URL = "https://7eminar.ua/news?type=all"
SOURCE_NAME = "7eminar.ua"


async def crawl_7eminar() -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π —Å 7eminar.ua
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ title, url, source, date, description
    """
    logger.info(f"üì∞ Fetching articles from {EMINAR_NEWS_URL}...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    news_items = []
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(EMINAR_NEWS_URL, headers=headers, timeout=30) as response:
                if response.status == 403:
                    logger.error("‚ùå Error: HTTP 403")
                    logger.error("‚ö†Ô∏è Site 7eminar.ua may be protected by CDN (Cloudflare)")
                    logger.error("üí° Possible solutions:")
                    logger.error("   1. Use Selenium/Playwright for browser automation")
                    logger.error("   2. Use proxy service")
                    logger.error("   3. Contact site administrators for API access")
                    return []
                
                if response.status != 200:
                    logger.error(f"‚ùå Error: HTTP {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ div.card-news__body
                article_blocks = soup.find_all('div', class_='card-news__body')
                
                logger.info(f"üìä Found {len(article_blocks)} article blocks")
                
                for article in article_blocks:
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        title_tag = article.find('h2', class_='card-news__title')
                        if not title_tag:
                            continue
                        
                        title = title_tag.get_text(strip=True)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
                        link_tag = article.find('a', class_='card-news__link')
                        if not link_tag:
                            continue
                        
                        url_path = link_tag.get('href', '')
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if url_path.startswith('/'):
                            full_url = f"https://7eminar.ua{url_path}"
                        elif not url_path.startswith('http'):
                            full_url = f"https://7eminar.ua/{url_path}"
                        else:
                            full_url = url_path
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                        description_div = article.find('div', class_='card-news__description')
                        description = ''
                        if description_div:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤–Ω—É—Ç—Ä–∏
                            paragraphs = description_div.find_all('p')
                            if paragraphs:
                                description = ' '.join(p.get_text(strip=True) for p in paragraphs)
                            else:
                                description = description_div.get_text(strip=True)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                        date_str = ''
                        date_div = article.find('div', class_='date-info')
                        if date_div:
                            date_str = date_div.get_text(strip=True)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                        category = ''
                        category_tag = article.find('a', class_='card-news__category')
                        if category_tag:
                            category = category_tag.get_text(strip=True)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                        image_url = ''
                        picture_tag = article.find('picture', class_='card-news__picture')
                        if picture_tag:
                            img_tag = picture_tag.find('img')
                            if img_tag:
                                image_url = img_tag.get('src', '')
                        
                        news_items.append({
                            'title': title,
                            'url': full_url,
                            'source': SOURCE_NAME,
                            'date': date_str,
                            'raw_date': date_str,
                            'description': description,
                            'category': category,
                            'image_url': image_url,
                        })
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error parsing article: {e}")
                        continue
                
                logger.info(f"‚úÖ Successfully parsed {len(news_items)} articles from 7eminar.ua")
                
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                for item in news_items:
                    parsed_date = parse_7eminar_date(item['raw_date'])
                    if parsed_date:
                        item['published_date'] = parsed_date.isoformat()
                    else:
                        item['published_date'] = datetime.now().isoformat()
                
                return news_items
                
    except aiohttp.ClientError as e:
        logger.error(f"‚ùå Network error: {e}")
        return []
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return []


def parse_7eminar_date(date_str: str) -> datetime:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π –¥–∞—Ç—ã —Å 7eminar.ua
    
    –ü—Ä–∏–º–µ—Ä—ã —Ñ–æ—Ä–º–∞—Ç–æ–≤:
    - "04.12.2025" -> –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
    - "27.11.2025" -> –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
    
    Args:
        date_str: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π
    
    Returns:
        datetime –æ–±—ä–µ–∫—Ç
    """
    if not date_str:
        return datetime.now()
    
    date_str = date_str.strip()
    now = datetime.now()
    
    try:
        # –§–æ—Ä–º–∞—Ç "04.12.2025" (DD.MM.YYYY)
        date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', date_str)
        if date_match:
            day = int(date_match.group(1))
            month = int(date_match.group(2))
            year = int(date_match.group(3))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤—Ä–µ–º—è
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_str)
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                return datetime(year, month, day, hour, minute)
            
            return datetime(year, month, day)
        
        # –§–æ—Ä–º–∞—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –º–µ—Å—è—Ü–µ–≤ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º
        month_map = {
            '—Å—ñ—á–Ω—è': 1, '—Å—ñ—á–µ–Ω—å': 1,
            '–ª—é—Ç–æ–≥–æ': 2, '–ª—é—Ç–∏–π': 2,
            '–±–µ—Ä–µ–∑–Ω—è': 3, '–±–µ—Ä–µ–∑–µ–Ω—å': 3,
            '–∫–≤—ñ—Ç–Ω—è': 4, '–∫–≤—ñ—Ç–µ–Ω—å': 4,
            '—Ç—Ä–∞–≤–Ω—è': 5, '—Ç—Ä–∞–≤–µ–Ω—å': 5,
            '—á–µ—Ä–≤–Ω—è': 6, '—á–µ—Ä–≤–µ–Ω—å': 6,
            '–ª–∏–ø–Ω—è': 7, '–ª–∏–ø–µ–Ω—å': 7,
            '—Å–µ—Ä–ø–Ω—è': 8, '—Å–µ—Ä–ø–µ–Ω—å': 8,
            '–≤–µ—Ä–µ—Å–Ω—è': 9, '–≤–µ—Ä–µ—Å–µ–Ω—å': 9,
            '–∂–æ–≤—Ç–Ω—è': 10, '–∂–æ–≤—Ç–µ–Ω—å': 10,
            '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–ª–∏—Å—Ç–æ–ø–∞–¥': 11,
            '–≥—Ä—É–¥–Ω—è': 12, '–≥—Ä—É–¥–µ–Ω—å': 12,
        }
        
        parts = date_str.split()
        if len(parts) >= 3:
            try:
                day = int(parts[0])
                month_name = parts[1].lower()
                year = int(parts[2])
                
                month = month_map.get(month_name)
                if month:
                    return datetime(year, month, day)
            except (ValueError, IndexError):
                pass
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not parse date '{date_str}': {e}")
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
    return now


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import asyncio
    
    async def main():
        news = await crawl_7eminar()
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π: {len(news)}")
        
        for i, item in enumerate(news[:10], 1):
            print(f"\n{i}. {item['title']}")
            print(f"   URL: {item['url']}")
            print(f"   –î–∞—Ç–∞: {item['raw_date']}")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {item['category']}")
            print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {item['description'][:100]}..." if item['description'] else "   –û–ø–∏—Å–∞–Ω–∏–µ: -")
    
    asyncio.run(main())

