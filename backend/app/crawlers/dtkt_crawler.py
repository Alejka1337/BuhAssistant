"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–π—Ç–∞ dtkt.ua (–î–µ–±–µ—Ç-–ö—Ä–µ–¥–∏—Ç)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç BeautifulSoup4 + aiohttp –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
"""

import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict
from datetime import datetime, timedelta
import logging
import re

logger = logging.getLogger(__name__)

# URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
DTKT_NEWS_URL = "https://news.dtkt.ua/?sort=main"
SOURCE_NAME = "dtkt.ua"


async def crawl_dtkt() -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å dtkt.ua
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ title, url, source, date
    """
    logger.info(f"üì∞ Fetching news from {DTKT_NEWS_URL}...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    news_items = []
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(DTKT_NEWS_URL, headers=headers, timeout=30) as response:
                if response.status == 403:
                    logger.error("‚ùå Error: HTTP 403")
                    logger.error("‚ö†Ô∏è Site dtkt.ua may be protected by CDN (Cloudflare/Akamai)")
                    logger.error("üí° Possible solutions:")
                    logger.error("   1. Use Selenium/Playwright for browser automation")
                    logger.error("   2. Use proxy service")
                    logger.error("   3. Contact site administrators for API access")
                    return []
                
                if response.status != 200:
                    logger.error(f"‚ùå Error: HTTP {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
                article_blocks = soup.find_all('div', class_='article-item')
                
                logger.info(f"üìä Found {len(article_blocks)} article blocks")
                
                for article in article_blocks:
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É
                        title_div = article.find('div', class_='article-item-title')
                        if not title_div:
                            continue
                        
                        link = title_div.find('a')
                        if not link:
                            continue
                        
                        title = link.get_text(strip=True)
                        url = link.get('href', '')
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if url.startswith('/'):
                            url = f"https://news.dtkt.ua{url}"
                        elif not url.startswith('http'):
                            url = f"https://news.dtkt.ua/{url}"
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                        info_div = article.find('div', class_='article-item-info')
                        date_str = ''
                        if info_div:
                            date_span = info_div.find('span', class_='date-info')
                            if date_span:
                                date_str = date_span.get_text(strip=True)
                        
                        news_items.append({
                            'title': title,
                            'url': url,
                            'source': SOURCE_NAME,
                            'date': date_str,
                            'raw_date': date_str,
                        })
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error parsing article: {e}")
                        continue
                
                logger.info(f"‚úÖ Successfully parsed {len(news_items)} articles from dtkt.ua")
                
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                for item in news_items:
                    parsed_date = parse_dtkt_date(item['raw_date'])
                    if parsed_date:
                        item['published_date'] = parsed_date.isoformat()
                    else:
                        item['published_date'] = datetime.now().isoformat()
                
                return news_items
                
    except aiohttp.ClientError as e:
        logger.error(f"‚ùå Network error: {e}")
        return []
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return []


def parse_dtkt_date(date_str: str) -> datetime:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π –¥–∞—Ç—ã —Å dtkt.ua
    
    –ü—Ä–∏–º–µ—Ä—ã —Ñ–æ—Ä–º–∞—Ç–æ–≤:
    - "–°—å–æ–≥–æ–¥–Ω—ñ 11:30" -> —Å–µ–≥–æ–¥–Ω—è
    - "–í—á–æ—Ä–∞ 15:45" -> –≤—á–µ—Ä–∞
    - "21.11.2025" -> –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –¥–∞—Ç–∞
    - "19.11.2025 10:30" -> –¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è
    
    Args:
        date_str: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π
    
    Returns:
        datetime –æ–±—ä–µ–∫—Ç
    """
    if not date_str:
        return datetime.now()
    
    date_str = date_str.strip()
    now = datetime.now()
    
    try:
        # "–°—å–æ–≥–æ–¥–Ω—ñ 11:30" –∏–ª–∏ "–°—å–æ–≥–æ–¥–Ω—ñ"
        if '–°—å–æ–≥–æ–¥–Ω—ñ' in date_str or '—Å—å–æ–≥–æ–¥–Ω—ñ' in date_str:
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_str)
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                return now.replace(hour=hour, minute=minute, second=0, microsecond=0)
            return now
        
        # "–í—á–æ—Ä–∞ 15:45" –∏–ª–∏ "–í—á–æ—Ä–∞"
        if '–í—á–æ—Ä–∞' in date_str or '–≤—á–æ—Ä–∞' in date_str:
            yesterday = now - timedelta(days=1)
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_str)
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                return yesterday.replace(hour=hour, minute=minute, second=0, microsecond=0)
            return yesterday
        
        # –§–æ—Ä–º–∞—Ç "21.11.2025" –∏–ª–∏ "21.11.2025 10:30"
        date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', date_str)
        if date_match:
            day = int(date_match.group(1))
            month = int(date_match.group(2))
            year = int(date_match.group(3))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤—Ä–µ–º—è
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_str)
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                return datetime(year, month, day, hour, minute)
            
            return datetime(year, month, day)
        
        # –§–æ—Ä–º–∞—Ç —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –º–µ—Å—è—Ü–µ–≤ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º
        month_map = {
            '—Å—ñ—á–Ω—è': 1, '—Å—ñ—á–µ–Ω—å': 1,
            '–ª—é—Ç–æ–≥–æ': 2, '–ª—é—Ç–∏–π': 2,
            '–±–µ—Ä–µ–∑–Ω—è': 3, '–±–µ—Ä–µ–∑–µ–Ω—å': 3,
            '–∫–≤—ñ—Ç–Ω—è': 4, '–∫–≤—ñ—Ç–µ–Ω—å': 4,
            '—Ç—Ä–∞–≤–Ω—è': 5, '—Ç—Ä–∞–≤–µ–Ω—å': 5,
            '—á–µ—Ä–≤–Ω—è': 6, '—á–µ—Ä–≤–µ–Ω—å': 6,
            '–ª–∏–ø–Ω—è': 7, '–ª–∏–ø–µ–Ω—å': 7,
            '—Å–µ—Ä–ø–Ω—è': 8, '—Å–µ—Ä–ø–µ–Ω—å': 8,
            '–≤–µ—Ä–µ—Å–Ω—è': 9, '–≤–µ—Ä–µ—Å–µ–Ω—å': 9,
            '–∂–æ–≤—Ç–Ω—è': 10, '–∂–æ–≤—Ç–µ–Ω—å': 10,
            '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–ª–∏—Å—Ç–æ–ø–∞–¥': 11,
            '–≥—Ä—É–¥–Ω—è': 12, '–≥—Ä—É–¥–µ–Ω—å': 12,
        }
        
        parts = date_str.split()
        if len(parts) >= 3:
            try:
                day = int(parts[0])
                month_name = parts[1].lower()
                year = int(parts[2])
                
                month = month_map.get(month_name)
                if month:
                    return datetime(year, month, day)
            except (ValueError, IndexError):
                pass
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not parse date '{date_str}': {e}")
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
    return now


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import asyncio
    
    async def main():
        news = await crawl_dtkt()
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news)}")
        for i, item in enumerate(news[:10], 1):
            print(f"\n{i}. {item['title']}")
            print(f"   URL: {item['url']}")
            print(f"   –î–∞—Ç–∞: {item['raw_date']}")
    
    asyncio.run(main())

