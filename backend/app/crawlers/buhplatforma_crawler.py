"""
–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ buhplatforma.com.ua
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç BeautifulSoup4 + aiohttp –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
"""

import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict
from datetime import datetime, timedelta
import logging
import re

logger = logging.getLogger(__name__)

# URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
BUHPLATFORMA_NEWS_URL = "https://buhplatforma.com.ua/news"
BUHPLATFORMA_ARTICLES_URL = "https://buhplatforma.com.ua/article"
SOURCE_NAME = "buhplatforma.com.ua"


async def crawl_buhplatforma() -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π —Å buhplatforma.com.ua
    
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ title, url, source, date, description
    """
    logger.info(f"üì∞ Fetching articles from buhplatforma.com.ua...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    news_items = []
    
    # –ü–∞—Ä—Å–∏–º –æ–±–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç—å–∏
    urls = [
        (BUHPLATFORMA_NEWS_URL, "news"),
        (BUHPLATFORMA_ARTICLES_URL, "articles")
    ]
    
    try:
        async with aiohttp.ClientSession() as session:
            for url, content_type in urls:
                logger.info(f"  üîç Parsing {content_type} from {url}...")
                
                try:
                    async with session.get(url, headers=headers, timeout=30) as response:
                        if response.status == 403:
                            logger.error(f"‚ùå Error: HTTP 403 for {url}")
                            logger.error("‚ö†Ô∏è Site buhplatforma.com.ua may be protected by CDN (Cloudflare)")
                            logger.error("üí° Possible solutions:")
                            logger.error("   1. Use Selenium/Playwright for browser automation")
                            logger.error("   2. Use proxy service")
                            logger.error("   3. Contact site administrators for API access")
                            continue
                        
                        if response.status != 200:
                            logger.error(f"‚ùå Error: HTTP {response.status} for {url}")
                            continue
                        
                        html = await response.text()
                        soup = BeautifulSoup(html, 'lxml')
                        
                        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏/—Å—Ç–∞—Ç—å—è–º–∏
                        news_list = soup.find('div', class_='news-list')
                        
                        if not news_list:
                            logger.warning(f"‚ö†Ô∏è Could not find div.news-list on {url}")
                            continue
                        
                        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ <article class="article">
                        article_blocks = news_list.find_all('article', class_='article')
                        
                        logger.info(f"  üìä Found {len(article_blocks)} {content_type} blocks")
                        
                        for article in article_blocks:
                            try:
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É
                                h4_tag = article.find('h4', class_='h4')
                                if not h4_tag:
                                    continue
                                
                                link = h4_tag.find('a')
                                if not link:
                                    continue
                                
                                title = link.get_text(strip=True)
                                url_path = link.get('href', '')
                                
                                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                                if url_path.startswith('/'):
                                    full_url = f"https://buhplatforma.com.ua{url_path}"
                                elif not url_path.startswith('http'):
                                    full_url = f"https://buhplatforma.com.ua/{url_path}"
                                else:
                                    full_url = url_path
                                
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                                description_div = article.find('div', class_='description')
                                description = description_div.get_text(strip=True) if description_div else ''
                                
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                                date_str = ''
                                time_tag = article.find('time', class_='time')
                                if time_tag:
                                    # –ü—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å datetime –∞—Ç—Ä–∏–±—É—Ç
                                    datetime_attr = time_tag.get('datetime', '')
                                    if datetime_attr:
                                        date_str = datetime_attr
                                    else:
                                        # –ï—Å–ª–∏ –Ω–µ—Ç datetime, –±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–∞
                                        date_str = time_tag.get_text(strip=True)
                                
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                                views = 0
                                views_div = article.find('div', class_='views')
                                if views_div:
                                    views_text = views_div.get_text(strip=True)
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "70503")
                                    views_match = re.search(r'(\d+)', views_text)
                                    if views_match:
                                        views = int(views_match.group(1))
                                
                                news_items.append({
                                    'title': title,
                                    'url': full_url,
                                    'source': SOURCE_NAME,
                                    'date': date_str,
                                    'raw_date': date_str,
                                    'description': description,
                                    'views': views,
                                    'content_type': content_type,
                                })
                                
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Error parsing article: {e}")
                                continue
                        
                        logger.info(f"  ‚úÖ Successfully parsed {len(article_blocks)} {content_type}")
                        
                except aiohttp.ClientError as e:
                    logger.error(f"‚ùå Network error for {url}: {e}")
                    continue
                except Exception as e:
                    logger.error(f"‚ùå Unexpected error for {url}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            logger.info(f"‚úÖ Total articles parsed from buhplatforma.com.ua: {len(news_items)}")
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            for item in news_items:
                parsed_date = parse_buhplatforma_date(item['raw_date'])
                if parsed_date:
                    item['published_date'] = parsed_date.isoformat()
                else:
                    item['published_date'] = datetime.now().isoformat()
            
            return news_items
                
    except aiohttp.ClientError as e:
        logger.error(f"‚ùå Network error: {e}")
        return []
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return []


def parse_buhplatforma_date(date_str: str) -> datetime:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π –¥–∞—Ç—ã —Å buhplatforma.com.ua
    
    –ü—Ä–∏–º–µ—Ä—ã —Ñ–æ—Ä–º–∞—Ç–æ–≤:
    - "2025-12-04T09:01:00+02:00" -> ISO 8601 —Å timezone
    - "4 –≥—Ä—É–¥–Ω—è 2025" -> –¥–∞—Ç–∞ —Å —É–∫—Ä–∞–∏–Ω—Å–∫–∏–º –º–µ—Å—è—Ü–µ–º
    - "–°—å–æ–≥–æ–¥–Ω—ñ 11:30" -> —Å–µ–≥–æ–¥–Ω—è
    - "–í—á–æ—Ä–∞ 15:45" -> –≤—á–µ—Ä–∞
    
    Args:
        date_str: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π
    
    Returns:
        datetime –æ–±—ä–µ–∫—Ç
    """
    if not date_str:
        return datetime.now()
    
    date_str = date_str.strip()
    now = datetime.now()
    
    try:
        # –§–æ—Ä–º–∞—Ç ISO 8601 (–∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ datetime)
        if 'T' in date_str and ('+' in date_str or 'Z' in date_str):
            # –£–¥–∞–ª—è–µ–º timezone info –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
            # –ü—Ä–∏–º–µ—Ä: "2025-12-04T09:01:00+02:00" -> "2025-12-04T09:01:00"
            clean_date = date_str.split('+')[0].split('Z')[0]
            return datetime.fromisoformat(clean_date)
        
        # "–°—å–æ–≥–æ–¥–Ω—ñ 11:30" –∏–ª–∏ "–°—å–æ–≥–æ–¥–Ω—ñ"
        if '–°—å–æ–≥–æ–¥–Ω—ñ' in date_str or '—Å—å–æ–≥–æ–¥–Ω—ñ' in date_str:
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_str)
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                return now.replace(hour=hour, minute=minute, second=0, microsecond=0)
            return now
        
        # "–í—á–æ—Ä–∞ 15:45" –∏–ª–∏ "–í—á–æ—Ä–∞"
        if '–í—á–æ—Ä–∞' in date_str or '–≤—á–æ—Ä–∞' in date_str:
            yesterday = now - timedelta(days=1)
            time_match = re.search(r'(\d{1,2}):(\d{2})', date_str)
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                return yesterday.replace(hour=hour, minute=minute, second=0, microsecond=0)
            return yesterday
        
        # –§–æ—Ä–º–∞—Ç "4 –≥—Ä—É–¥–Ω—è 2025" –∏–ª–∏ "21 –ª–∏—Å—Ç–æ–ø–∞–¥–∞ 2025"
        month_map = {
            '—Å—ñ—á–Ω—è': 1, '—Å—ñ—á–µ–Ω—å': 1,
            '–ª—é—Ç–æ–≥–æ': 2, '–ª—é—Ç–∏–π': 2,
            '–±–µ—Ä–µ–∑–Ω—è': 3, '–±–µ—Ä–µ–∑–µ–Ω—å': 3,
            '–∫–≤—ñ—Ç–Ω—è': 4, '–∫–≤—ñ—Ç–µ–Ω—å': 4,
            '—Ç—Ä–∞–≤–Ω—è': 5, '—Ç—Ä–∞–≤–µ–Ω—å': 5,
            '—á–µ—Ä–≤–Ω—è': 6, '—á–µ—Ä–≤–µ–Ω—å': 6,
            '–ª–∏–ø–Ω—è': 7, '–ª–∏–ø–µ–Ω—å': 7,
            '—Å–µ—Ä–ø–Ω—è': 8, '—Å–µ—Ä–ø–µ–Ω—å': 8,
            '–≤–µ—Ä–µ—Å–Ω—è': 9, '–≤–µ—Ä–µ—Å–µ–Ω—å': 9,
            '–∂–æ–≤—Ç–Ω—è': 10, '–∂–æ–≤—Ç–µ–Ω—å': 10,
            '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–ª–∏—Å—Ç–æ–ø–∞–¥': 11,
            '–≥—Ä—É–¥–Ω—è': 12, '–≥—Ä—É–¥–µ–Ω—å': 12,
        }
        
        parts = date_str.split()
        if len(parts) >= 3:
            try:
                day = int(parts[0])
                month_name = parts[1].lower()
                year = int(parts[2])
                
                month = month_map.get(month_name)
                if month:
                    return datetime(year, month, day)
            except (ValueError, IndexError):
                pass
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not parse date '{date_str}': {e}")
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
    return now


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import asyncio
    
    async def main():
        news = await crawl_buhplatforma()
        print(f"\n–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å—Ç–∞—Ç–µ–π: {len(news)}")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        news_count = sum(1 for item in news if item['content_type'] == 'news')
        articles_count = sum(1 for item in news if item['content_type'] == 'articles')
        
        print(f"  - –ù–æ–≤–æ—Å—Ç–µ–π: {news_count}")
        print(f"  - –°—Ç–∞—Ç–µ–π: {articles_count}")
        
        for i, item in enumerate(news[:5], 1):
            print(f"\n{i}. [{item['content_type']}] {item['title']}")
            print(f"   URL: {item['url']}")
            print(f"   –î–∞—Ç–∞: {item['raw_date']}")
            print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {item['description'][:100]}...")
            print(f"   –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {item['views']}")
    
    asyncio.run(main())

