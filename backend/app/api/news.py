"""
News API endpoints
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from sqlalchemy import desc
from app.db.database import get_db
from app.models.news import News
from app.services.minfin_crawler import crawl_minfin
from app.services.tax_gov_ua_crawler import crawl_tax_gov_ua
from app.services.liga_net_crawler import crawl_liga_net
from app.services.buhgalter911_crawler import crawl_buhgalter911
from app.services.news_filter import filter_relevant_news
from app.crawlers.tax_gov_ua_playwright import crawl_tax_gov_ua as crawl_tax_gov_ua_playwright
from app.crawlers.diia_gov_ua_playwright import crawl_diia_gov_ua as crawl_diia_gov_ua_playwright
from app.crawlers.dtkt_crawler import crawl_dtkt
from typing import List, Optional
from datetime import datetime

router = APIRouter()


@router.post("/crawl/minfin")
async def crawl_minfin_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è minfin.com.ua
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å /ua/articles/ –∏ /ua/news/
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    print("üï∑Ô∏è Starting Minfin crawler...")
    
    try:
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥
        articles = await crawl_minfin()
        
        if not articles:
            return {
                "status": "success",
                "message": "No new articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        articles_dict = [a.to_dict() for a in articles]
        filtered_articles = await filter_relevant_news(articles_dict)
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º summary –∫–∞–∫ content
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],  # JSON array
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow(),  # –£ Minfin –Ω–µ—Ç –¥–∞—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"Crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ Crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl/tax-gov-ua")
async def crawl_tax_gov_ua_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è tax.gov.ua
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å https://tax.gov.ua/media-tsentr/novini/
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        print("üï∑Ô∏è Starting Tax.gov.ua crawler...")
        
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        articles = await crawl_tax_gov_ua()
        print(f"üì∞ Crawled {len(articles)} articles")
        
        if not articles:
            return {
                "status": "warning",
                "message": "No articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0,
                "skipped": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        articles_dict = [a.to_dict() for a in articles]
        filtered_articles = await filter_relevant_news(articles_dict)
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow(),
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"Crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ Crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl/liga-net")
async def crawl_liga_net_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è liga.net
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å https://news.liga.net/ua
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        print("üï∑Ô∏è Starting Liga.net crawler...")
        
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        articles = await crawl_liga_net()
        print(f"üì∞ Crawled {len(articles)} articles")
        
        if not articles:
            return {
                "status": "warning",
                "message": "No articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0,
                "skipped": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        articles_dict = [a.to_dict() for a in articles]
        filtered_articles = await filter_relevant_news(articles_dict)
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', article.get('categories', ['–∑–∞–≥–∞–ª—å–Ω–µ'])[0] if isinstance(article.get('categories'), list) else '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow(),
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"Crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ Crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl/tax-gov-ua-playwright")
async def crawl_tax_gov_ua_playwright_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å Playwright –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è tax.gov.ua
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å https://tax.gov.ua/media-tsentr/novini/ –∏—Å–ø–æ–ª—å–∑—É—è –±—Ä–∞—É–∑–µ—Ä
    –û–±—Ö–æ–¥–∏—Ç CDN –∑–∞—â–∏—Ç—É —á–µ—Ä–µ–∑ Playwright
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        print("üï∑Ô∏è Starting Tax.gov.ua Playwright crawler...")
        
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Playwright
        articles = await crawl_tax_gov_ua_playwright()
        print(f"üì∞ Crawled {len(articles)} articles with Playwright")
        
        if not articles:
            return {
                "status": "warning",
                "message": "No articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0,
                "skipped": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        filtered_articles = await filter_relevant_news(articles)
        print(f"‚úÖ Filtered {len(filtered_articles)} relevant articles")
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            try:
                if 'published_date' in article and article['published_date']:
                    published_at = datetime.fromisoformat(article['published_date'])
                else:
                    published_at = datetime.utcnow()
            except:
                published_at = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article.get('source', 'tax.gov.ua'),
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=published_at,
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"Playwright crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ Playwright crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Playwright crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl/diia-gov-ua-playwright")
async def crawl_diia_gov_ua_playwright_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å Playwright –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è diia.gov.ua
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å https://diia.gov.ua/news –∏—Å–ø–æ–ª—å–∑—É—è –±—Ä–∞—É–∑–µ—Ä
    –û–±—Ö–æ–¥–∏—Ç SPA –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        print("üï∑Ô∏è Starting Diia.gov.ua Playwright crawler...")
        
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Playwright
        articles = await crawl_diia_gov_ua_playwright()
        print(f"üì∞ Crawled {len(articles)} articles from Diia with Playwright")
        
        if not articles:
            return {
                "status": "warning",
                "message": "No articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0,
                "skipped": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        filtered_articles = await filter_relevant_news(articles)
        print(f"‚úÖ Filtered {len(filtered_articles)} relevant articles for business/FOP")
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            try:
                if 'published_date' in article and article['published_date']:
                    published_at = datetime.fromisoformat(article['published_date'])
                else:
                    published_at = datetime.utcnow()
            except:
                published_at = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article.get('source', 'diia.gov.ua'),
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=published_at,
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"Diia Playwright crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ Diia Playwright crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Diia Playwright crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl/dtkt")
async def crawl_dtkt_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è dtkt.ua (–î–µ–±–µ—Ç-–ö—Ä–µ–¥–∏—Ç)
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å https://news.dtkt.ua/?sort=main
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        print("üï∑Ô∏è Starting dtkt.ua crawler...")
        
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        articles = await crawl_dtkt()
        print(f"üì∞ Crawled {len(articles)} articles from dtkt.ua")
        
        if not articles:
            return {
                "status": "warning",
                "message": "No articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0,
                "skipped": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        filtered_articles = await filter_relevant_news(articles)
        print(f"‚úÖ Filtered {len(filtered_articles)} relevant articles")
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            try:
                if 'published_date' in article and article['published_date']:
                    published_at = datetime.fromisoformat(article['published_date'])
                else:
                    published_at = datetime.utcnow()
            except:
                published_at = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article.get('source', 'dtkt.ua'),
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=published_at,
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"dtkt.ua crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ dtkt.ua crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå dtkt.ua crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/crawl/buhgalter911")
async def crawl_buhgalter911_news(db: Session = Depends(get_db)):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫—Ä–∞—É–ª–µ—Ä –¥–ª—è buhgalter911.com
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å https://buhgalter911.com/uk/news/
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI API
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
    """
    try:
        print("üï∑Ô∏è Starting Buhgalter911.com crawler...")
        
        # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        articles = await crawl_buhgalter911()
        print(f"üì∞ Crawled {len(articles)} articles")
        
        if not articles:
            return {
                "status": "warning",
                "message": "No articles found",
                "parsed": 0,
                "filtered": 0,
                "saved": 0,
                "skipped": 0
            }
        
        # –®–∞–≥ 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        articles_dict = [a.to_dict() for a in articles]
        filtered_articles = await filter_relevant_news(articles_dict)
        
        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_articles:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π —Å—Ç–∞—Ç—å–∏
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                print(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', article.get('categories', ['–∑–∞–≥–∞–ª—å–Ω–µ'])[0] if isinstance(article.get('categories'), list) else '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow(),
            )
            
            db.add(news_item)
            saved_count += 1
            print(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            "status": "success",
            "message": f"Crawler finished successfully",
            "parsed": len(articles),
            "filtered": len(filtered_articles),
            "saved": saved_count,
            "skipped": skipped_count
        }
        
        print(f"‚úÖ Crawler finished: {result}")
        return result
        
    except Exception as e:
        print(f"‚ùå Crawler error: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/")
async def get_news(
    category: Optional[str] = Query(None, description="–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"),
    target_audience: Optional[str] = Query(None, description="–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è (–§–û–ü, –Æ–û, –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏)"),
    limit: int = Query(20, ge=1, le=100, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π"),
    offset: int = Query(0, ge=0, description="–°–º–µ—â–µ–Ω–∏–µ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"),
    db: Session = Depends(get_db)
):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    
    - **category**: –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–ø–æ–¥–∞—Ç–∫–∏, –∑–≤—ñ—Ç–Ω—ñ—Å—Ç—å, –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–æ, –Ñ–°–í, –∑–∞—Ä–ø–ª–∞—Ç–∞, –±—É—Ö–æ–±–ª—ñ–∫)
    - **target_audience**: –§–∏–ª—å—Ç—Ä –ø–æ –∞—É–¥–∏—Ç–æ—Ä–∏–∏ (–§–û–ü, –Æ–û, –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏)
    - **limit**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
    - **offset**: –°–º–µ—â–µ–Ω–∏–µ –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    """
    query = db.query(News).filter(News.is_published == True)
    
    if category:
        # PostgreSQL JSON array contains
        query = query.filter(News.categories.contains([category]))
    
    if target_audience:
        # PostgreSQL JSON array contains
        query = query.filter(News.target_audience.contains([target_audience]))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
    query = query.order_by(desc(News.published_at))
    
    # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
    total = query.count()
    news_items = query.offset(offset).limit(limit).all()
    
    return {
        "total": total,
        "limit": limit,
        "offset": offset,
        "items": [
            {
                "id": item.id,
                "title": item.title,
                "url": item.url,
                "source": item.source,
                "categories": item.categories,
                "target_audience": item.target_audience,
                "summary": item.summary,
                "published_at": item.published_at.isoformat() if item.published_at else None,
                "created_at": item.created_at.isoformat() if item.created_at else None,
            }
            for item in news_items
        ]
    }


@router.get("/categories")
async def get_categories(db: Session = Depends(get_db)):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
    news_items = db.query(News).filter(News.is_published == True).all()
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Ä—É—á–Ω—É—é, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ JSON –ø–æ–ª–µ
    category_counts = {}
    for item in news_items:
        if item.categories:
            for cat in item.categories:
                category_counts[cat] = category_counts.get(cat, 0) + 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
    sorted_categories = sorted(
        category_counts.items(),
        key=lambda x: x[1],
        reverse=True
    )
    
    return {
        "categories": [
            {"name": cat, "count": count}
            for cat, count in sorted_categories
        ]
    }


@router.get("/stats")
async def news_stats(db: Session = Depends(get_db)):
    """
    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º
    """
    from sqlalchemy import func
    
    total_news = db.query(News).count()
    published_news = db.query(News).filter(News.is_published == True).count()
    
    # –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    by_source = (
        db.query(News.source, func.count(News.id).label('count'))
        .group_by(News.source)
        .all()
    )
    
    # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–≤—Ä—É—á–Ω—É—é, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ JSON)
    news_items = db.query(News).filter(News.is_published == True).all()
    category_counts = {}
    for item in news_items:
        if item.categories:
            for cat in item.categories:
                category_counts[cat] = category_counts.get(cat, 0) + 1
    
    # –¢–æ–ø-10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    top_categories = sorted(
        category_counts.items(),
        key=lambda x: x[1],
        reverse=True
    )[:10]
    
    return {
        "total_news": total_news,
        "published_news": published_news,
        "by_source": [
            {"source": source, "count": count}
            for source, count in by_source
        ],
        "top_categories": [
            {"category": cat, "count": count}
            for cat, count in top_categories
        ]
    }

