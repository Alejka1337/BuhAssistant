"""
Celery tasks –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
"""
import logging
import asyncio
from celery import shared_task
from sqlalchemy.orm import Session

from app.db.database import SessionLocal
from app.services.minfin_crawler import MinfinCrawler
from app.services.liga_net_crawler import LigaNetCrawler
from app.services.buhgalter911_crawler import Buhgalter911Crawler
from app.services.news_filter import NewsFilterService, filter_relevant_news
from app.models.news import News
from app.core.config import settings
from datetime import datetime

# –ù–æ–≤—ã–µ Playwright –ø–∞—Ä—Å–µ—Ä—ã
from app.crawlers.tax_gov_ua_playwright import crawl_tax_gov_ua as crawl_tax_gov_ua_playwright
from app.crawlers.diia_gov_ua_playwright import crawl_diia_gov_ua as crawl_diia_gov_ua_playwright
from app.crawlers.dtkt_crawler import crawl_dtkt

logger = logging.getLogger(__name__)


@shared_task(name="crawl_minfin_news_task")
def crawl_minfin_news_task():
    """
    Celery task –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å minfin.com.ua
    """
    logger.info("üï∑Ô∏è Starting scheduled Minfin crawler task...")
    
    db: Session = SessionLocal()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        crawler = MinfinCrawler()
        all_news = crawler.crawl_all()
        
        logger.info(f"üì∞ Crawled {len(all_news)} news items")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        filter_service = NewsFilterService(api_key=settings.OPENAI_API_KEY)
        filtered_news = filter_service.filter_relevant_news(all_news)
        
        logger.info(f"‚úÖ OpenAI filtered: {len(filtered_news)}/{len(all_news)} relevant articles")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_news:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                logger.info(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=article.get('categories', []),
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow()
            )
            
            db.add(news_item)
            saved_count += 1
            logger.info(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            'status': 'success',
            'parsed': len(all_news),
            'filtered': len(filtered_news),
            'saved': saved_count,
            'skipped': skipped_count
        }
        
        logger.info(f"‚úÖ Scheduled crawler task finished: {result}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Scheduled crawler task error: {str(e)}")
        db.rollback()
        raise
    finally:
        db.close()


@shared_task(name="crawl_liga_net_news_task")
def crawl_liga_net_news_task():
    """
    Celery task –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å liga.net
    """
    logger.info("üï∑Ô∏è Starting scheduled Liga.net crawler task...")
    
    db: Session = SessionLocal()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        crawler = LigaNetCrawler()
        all_news = crawler.crawl_all()
        
        logger.info(f"üì∞ Crawled {len(all_news)} news items from Liga.net")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        filter_service = NewsFilterService(api_key=settings.OPENAI_API_KEY)
        filtered_news = filter_service.filter_relevant_news(all_news)
        
        logger.info(f"‚úÖ OpenAI filtered: {len(filtered_news)}/{len(all_news)} relevant articles")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_news:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                logger.info(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=article.get('categories', []),
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow()
            )
            
            db.add(news_item)
            saved_count += 1
            logger.info(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            'status': 'success',
            'source': 'liga.net',
            'parsed': len(all_news),
            'filtered': len(filtered_news),
            'saved': saved_count,
            'skipped': skipped_count
        }
        
        logger.info(f"‚úÖ Scheduled Liga.net crawler task finished: {result}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Scheduled Liga.net crawler task error: {str(e)}")
        db.rollback()
        raise
    finally:
        db.close()


@shared_task(name="crawl_buhgalter911_news_task")
def crawl_buhgalter911_news_task():
    """
    Celery task –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å buhgalter911.com
    """
    logger.info("üï∑Ô∏è Starting scheduled Buhgalter911.com crawler task...")
    
    db: Session = SessionLocal()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
        crawler = Buhgalter911Crawler()
        all_news = crawler.crawl_all()
        
        logger.info(f"üì∞ Crawled {len(all_news)} news items from Buhgalter911.com")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI
        filter_service = NewsFilterService(api_key=settings.OPENAI_API_KEY)
        filtered_news = filter_service.filter_relevant_news(all_news)
        
        logger.info(f"‚úÖ OpenAI filtered: {len(filtered_news)}/{len(all_news)} relevant articles")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_news:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                logger.info(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article['source'],
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=article.get('categories', []),
                target_audience=article.get('target_audience', []),
                published_at=datetime.utcnow()
            )
            
            db.add(news_item)
            saved_count += 1
            logger.info(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            'status': 'success',
            'source': 'buhgalter911.com',
            'parsed': len(all_news),
            'filtered': len(filtered_news),
            'saved': saved_count,
            'skipped': skipped_count
        }
        
        logger.info(f"‚úÖ Scheduled Buhgalter911.com crawler task finished: {result}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Scheduled Buhgalter911.com crawler task error: {str(e)}")
        db.rollback()
        raise
    finally:
        db.close()


@shared_task(name="crawl_tax_gov_ua_playwright_task")
def crawl_tax_gov_ua_playwright_task():
    """
    Celery task –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å tax.gov.ua —á–µ—Ä–µ–∑ Playwright
    """
    logger.info("üï∑Ô∏è Starting scheduled Tax.gov.ua Playwright crawler task...")
    
    db: Session = SessionLocal()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Playwright (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        all_news = loop.run_until_complete(crawl_tax_gov_ua_playwright())
        loop.close()
        
        logger.info(f"üì∞ Crawled {len(all_news)} news items from Tax.gov.ua")
        
        if not all_news:
            return {
                'status': 'warning',
                'source': 'tax.gov.ua',
                'parsed': 0,
                'filtered': 0,
                'saved': 0,
                'skipped': 0
            }
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        filtered_news = loop.run_until_complete(filter_relevant_news(all_news))
        loop.close()
        
        logger.info(f"‚úÖ OpenAI filtered: {len(filtered_news)}/{len(all_news)} relevant articles")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_news:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                logger.info(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            try:
                if 'published_date' in article and article['published_date']:
                    published_at = datetime.fromisoformat(article['published_date'])
                else:
                    published_at = datetime.utcnow()
            except:
                published_at = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article.get('source', 'tax.gov.ua'),
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=published_at
            )
            
            db.add(news_item)
            saved_count += 1
            logger.info(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            'status': 'success',
            'source': 'tax.gov.ua',
            'parsed': len(all_news),
            'filtered': len(filtered_news),
            'saved': saved_count,
            'skipped': skipped_count
        }
        
        logger.info(f"‚úÖ Scheduled Tax.gov.ua Playwright crawler task finished: {result}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Scheduled Tax.gov.ua Playwright crawler task error: {str(e)}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise
    finally:
        db.close()


@shared_task(name="crawl_diia_gov_ua_playwright_task")
def crawl_diia_gov_ua_playwright_task():
    """
    Celery task –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å diia.gov.ua —á–µ—Ä–µ–∑ Playwright
    """
    logger.info("üï∑Ô∏è Starting scheduled Diia.gov.ua Playwright crawler task...")
    
    db: Session = SessionLocal()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Playwright (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        all_news = loop.run_until_complete(crawl_diia_gov_ua_playwright())
        loop.close()
        
        logger.info(f"üì∞ Crawled {len(all_news)} news items from Diia.gov.ua")
        
        if not all_news:
            return {
                'status': 'warning',
                'source': 'diia.gov.ua',
                'parsed': 0,
                'filtered': 0,
                'saved': 0,
                'skipped': 0
            }
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        filtered_news = loop.run_until_complete(filter_relevant_news(all_news))
        loop.close()
        
        logger.info(f"‚úÖ OpenAI filtered: {len(filtered_news)}/{len(all_news)} relevant articles")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_news:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                logger.info(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            try:
                if 'published_date' in article and article['published_date']:
                    published_at = datetime.fromisoformat(article['published_date'])
                else:
                    published_at = datetime.utcnow()
            except:
                published_at = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article.get('source', 'diia.gov.ua'),
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=published_at
            )
            
            db.add(news_item)
            saved_count += 1
            logger.info(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            'status': 'success',
            'source': 'diia.gov.ua',
            'parsed': len(all_news),
            'filtered': len(filtered_news),
            'saved': saved_count,
            'skipped': skipped_count
        }
        
        logger.info(f"‚úÖ Scheduled Diia.gov.ua Playwright crawler task finished: {result}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Scheduled Diia.gov.ua Playwright crawler task error: {str(e)}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise
    finally:
        db.close()


@shared_task(name="crawl_dtkt_task")
def crawl_dtkt_task():
    """
    Celery task –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å dtkt.ua
    """
    logger.info("üï∑Ô∏è Starting scheduled dtkt.ua crawler task...")
    
    db: Session = SessionLocal()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        all_news = loop.run_until_complete(crawl_dtkt())
        loop.close()
        
        logger.info(f"üì∞ Crawled {len(all_news)} news items from dtkt.ua")
        
        if not all_news:
            return {
                'status': 'warning',
                'source': 'dtkt.ua',
                'parsed': 0,
                'filtered': 0,
                'saved': 0,
                'skipped': 0
            }
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        filtered_news = loop.run_until_complete(filter_relevant_news(all_news))
        loop.close()
        
        logger.info(f"‚úÖ OpenAI filtered: {len(filtered_news)}/{len(all_news)} relevant articles")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        saved_count = 0
        skipped_count = 0
        
        for article in filtered_news:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            existing = db.query(News).filter(News.url == article['url']).first()
            
            if existing:
                logger.info(f"  ‚è≠Ô∏è Skipping duplicate: {article['title'][:50]}...")
                skipped_count += 1
                continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            try:
                if 'published_date' in article and article['published_date']:
                    published_at = datetime.fromisoformat(article['published_date'])
                else:
                    published_at = datetime.utcnow()
            except:
                published_at = datetime.utcnow()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            news_item = News(
                title=article['title'],
                url=article['url'],
                source=article.get('source', 'dtkt.ua'),
                content=article.get('summary', article['title']),
                summary=article.get('summary', article['title']),
                categories=[article.get('category', '–∑–∞–≥–∞–ª—å–Ω–µ')],
                target_audience=article.get('target_audience', []),
                published_at=published_at
            )
            
            db.add(news_item)
            saved_count += 1
            logger.info(f"  üíæ Saved: {article['title'][:50]}...")
        
        db.commit()
        
        result = {
            'status': 'success',
            'source': 'dtkt.ua',
            'parsed': len(all_news),
            'filtered': len(filtered_news),
            'saved': saved_count,
            'skipped': skipped_count
        }
        
        logger.info(f"‚úÖ Scheduled dtkt.ua crawler task finished: {result}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Scheduled dtkt.ua crawler task error: {str(e)}")
        import traceback
        traceback.print_exc()
        db.rollback()
        raise
    finally:
        db.close()


@shared_task(name="crawl_all_news_sources_task")
def crawl_all_news_sources_task():
    """
    Celery task –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –í–°–ï–• –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
    
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ –∫—Ä–∞—É–ª–µ—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ:
    1. Minfin.com.ua (BS4)
    2. Liga.net (BS4)
    3. Buhgalter911.com (BS4)
    4. Tax.gov.ua (Playwright) ‚≠ê NEW
    5. Diia.gov.ua (Playwright) ‚≠ê NEW
    6. Dtkt.ua (BS4) ‚≠ê NEW
    
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞:
    - –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏
    - –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–µ—Ä–µ–∑ OpenAI
    - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ë–î
    """
    logger.info("=" * 80)
    logger.info(f"üï∑Ô∏è Starting FULL NEWS CRAWL at {datetime.utcnow().isoformat()}")
    logger.info("=" * 80)
    
    results = []
    
    # 1. Minfin.com.ua
    try:
        logger.info("\nüì∞ [1/6] Crawling Minfin.com.ua...")
        minfin_result = crawl_minfin_news_task()
        results.append(minfin_result)
        logger.info(f"‚úÖ Minfin: {minfin_result}")
    except Exception as e:
        logger.error(f"‚ùå Minfin crawler failed: {str(e)}")
        results.append({'status': 'error', 'source': 'minfin.com.ua', 'error': str(e)})
    
    # 2. Liga.net
    try:
        logger.info("\nüì∞ [2/6] Crawling Liga.net...")
        liga_result = crawl_liga_net_news_task()
        results.append(liga_result)
        logger.info(f"‚úÖ Liga.net: {liga_result}")
    except Exception as e:
        logger.error(f"‚ùå Liga.net crawler failed: {str(e)}")
        results.append({'status': 'error', 'source': 'liga.net', 'error': str(e)})
    
    # 3. Buhgalter911.com
    try:
        logger.info("\nüì∞ [3/6] Crawling Buhgalter911.com...")
        buhgalter_result = crawl_buhgalter911_news_task()
        results.append(buhgalter_result)
        logger.info(f"‚úÖ Buhgalter911.com: {buhgalter_result}")
    except Exception as e:
        logger.error(f"‚ùå Buhgalter911.com crawler failed: {str(e)}")
        results.append({'status': 'error', 'source': 'buhgalter911.com', 'error': str(e)})
    
    # 4. Tax.gov.ua (Playwright) ‚≠ê NEW
    try:
        logger.info("\nüì∞ [4/6] Crawling Tax.gov.ua (Playwright)...")
        tax_result = crawl_tax_gov_ua_playwright_task()
        results.append(tax_result)
        logger.info(f"‚úÖ Tax.gov.ua: {tax_result}")
    except Exception as e:
        logger.error(f"‚ùå Tax.gov.ua Playwright crawler failed: {str(e)}")
        results.append({'status': 'error', 'source': 'tax.gov.ua', 'error': str(e)})
    
    # 5. Diia.gov.ua (Playwright) ‚≠ê NEW
    try:
        logger.info("\nüì∞ [5/6] Crawling Diia.gov.ua (Playwright)...")
        diia_result = crawl_diia_gov_ua_playwright_task()
        results.append(diia_result)
        logger.info(f"‚úÖ Diia.gov.ua: {diia_result}")
    except Exception as e:
        logger.error(f"‚ùå Diia.gov.ua Playwright crawler failed: {str(e)}")
        results.append({'status': 'error', 'source': 'diia.gov.ua', 'error': str(e)})
    
    # 6. Dtkt.ua (BS4) ‚≠ê NEW
    try:
        logger.info("\nüì∞ [6/6] Crawling Dtkt.ua...")
        dtkt_result = crawl_dtkt_task()
        results.append(dtkt_result)
        logger.info(f"‚úÖ Dtkt.ua: {dtkt_result}")
    except Exception as e:
        logger.error(f"‚ùå Dtkt.ua crawler failed: {str(e)}")
        results.append({'status': 'error', 'source': 'dtkt.ua', 'error': str(e)})
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_parsed = sum(r.get('parsed', 0) for r in results)
    total_filtered = sum(r.get('filtered', 0) for r in results)
    total_saved = sum(r.get('saved', 0) for r in results)
    total_skipped = sum(r.get('skipped', 0) for r in results)
    
    summary = {
        'status': 'success',
        'timestamp': datetime.utcnow().isoformat(),
        'sources_crawled': len(results),
        'total_parsed': total_parsed,
        'total_filtered': total_filtered,
        'total_saved': total_saved,
        'total_skipped': total_skipped,
        'results': results
    }
    
    logger.info("=" * 80)
    logger.info(f"üéâ FULL NEWS CRAWL COMPLETED")
    logger.info(f"   Total parsed: {total_parsed}")
    logger.info(f"   Total filtered by OpenAI: {total_filtered}")
    logger.info(f"   Total saved to DB: {total_saved}")
    logger.info(f"   Total skipped (duplicates): {total_skipped}")
    logger.info("=" * 80)
    
    return summary


@shared_task(name="test_celery_task")
def test_celery_task():
    """
    –¢–µ—Å—Ç–æ–≤–∞—è Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã
    """
    logger.info("‚úÖ Test Celery task executed successfully!")
    return {"status": "success", "message": "Celery is working!"}

